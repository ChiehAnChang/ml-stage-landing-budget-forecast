On the technical side, I reviewed and learned how to use the `requests` library to send RESTful API requests to retrieve data from a company's API. I also learned how to convert the response object into JSON format using either `pandas.DataFrame()` or `pandas.json_normalize()`. The `normalize` method flattens nested JSON structures into a simpler format. I learned how to configure pandas display settings using `pandas.set_option()` and reset them with `pandas.reset_option()`.

Additionally, I explored the power of the `map()` function with lambda function for data manipulation and gained a better understanding of how indexing works in pandas. For example, `data[single_variable]` returns a Series (a single column), `data[[list_of_columns]]` returns a DataFrame, and `data[boolean_condition]` filters rows based on the condition. I also used the `apply()` method on DataFrames, where setting `axis=1` allows you to apply a function to each row. Lastly, I learned how the `pandas.to_datetime()` function works for converting strings to datetime objects.

Comparing with university's project, in most of those cases, the data provided was relatively clean or came with explanations of the parameters. In this project, I truly understood what it means to be a "domain expert." I spent a lot of time understanding the meanings of the variables and the relevant terminology. I also learned how difficult it can be to collect data. The data I needed was often not available in a single endpoint. Sometimes, I had to use data from one endpoint to retrieve data from another. Although it was a bit tedious, I enjoyed the process. It also reflected a real-world scenario where large datasets are often separated into different endpoints (i.e., different tables in a database) and need to be joined using IDs (i.e., primary/foreign keys).